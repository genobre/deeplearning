{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm\n",
    "Author: Geovanna Santos Nobre de Oliveira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import logistic \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement just one step of the gradient descent so it can be clear what is happening in the algorithm.\n",
    "\n",
    "**Warning:** This code does not work!!! Some of the variables here, such as x, y, b and w are not declared, so don't try to run it because it will not work.\n",
    "\n",
    "Also, this is just a **step** of the gradient descent, it is not the whole algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are considering a training set with size m, where each training example has 2 variables (n=2)\n",
    "\n",
    "# Initializing variables\n",
    "j = 0                       # cost \n",
    "dw = np.array([0, 0])       # accumulator of the derivative oj J as a function of w\n",
    "db = 0                      # accumulator of the derivative oj J as a function of b\n",
    "\n",
    "# One step of the gradient descent\n",
    "for i in range(m):\n",
    "    z[i] = int(np.dot(w.transpose(),x[:,i])) + b            # linear function of x\n",
    "    a[i] = logistic.cdf(z[i])                               # Sigmoid function\n",
    "    \n",
    "    j += y[i]*math.log(a[i]) + (1-y[i])*math.log(1-a[i])    # loss function added to the cost\n",
    "    \n",
    "    dz[i] = a[i] - y[i]                                     # derivative oj J as a function of z\n",
    "    \n",
    "    dw[0] += x[0,i]*dz[i]                                   # derivative oj J as a function of w0\n",
    "    dw[1] += x[1,i]*dz[i]                                   # derivative oj J as a function of w1\n",
    "    \n",
    "    db += dz[i]                                             # derivative oj J as a function of b\n",
    "    \n",
    "# taking the means of the accumulators\n",
    "j = j/m\n",
    "dw = dw/m\n",
    "db = b/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized implementation of dw, with dw having $n$ features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are considering a training set with size m, where each training example has n variables\n",
    "\n",
    "# Initializing variables\n",
    "j = 0                       # cost \n",
    "dw = np.zeros((n, 1))       # accumulator of the derivative oj J as a function of w\n",
    "db = 0                      # accumulator of the derivative oj J as a function of b\n",
    "\n",
    "# One step of the gradient descent\n",
    "for i in range(m):\n",
    "    z[i] = int(np.dot(w.transpose(),x[:,i])) + b            # linear function of x\n",
    "    a[i] = logistic.cdf(z[i])                               # Sigmoid function\n",
    "    \n",
    "    j += y[i]*math.log(a[i]) + (1-y[i])*math.log(1-a[i])    # loss function added to the cost\n",
    "    \n",
    "    dz[i] = a[i] - y[i]                                     # derivative oj J as a function of z\n",
    "    \n",
    "    dw += x[i]*dz[i]                                        # derivative oj J as a function of w\n",
    "    \n",
    "    db += dz[i]                                             # derivative oj J as a function of b\n",
    "    \n",
    "# taking the means of the accumulators\n",
    "j = j/m\n",
    "dw = dw/m\n",
    "db = b/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.dot(w.transpose(),X) + b                             # linear function of x\n",
    "A = logistic.cdf(Z)                                         # Sigmoid function\n",
    "\n",
    "dZ = A - Y                                                  # derivative oj J as a function of z\n",
    "\n",
    "dw = 1/m * np.dot(X, dZ.transpose())\n",
    "db = 1/m * np.sum(dZ)\n",
    "\n",
    "w = w - alpha*dw\n",
    "b = b - alpha*db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c9ae84327cd6385aaafa8385bf22efb0b44f1d85c6039a8f31b6c6394ece0b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
